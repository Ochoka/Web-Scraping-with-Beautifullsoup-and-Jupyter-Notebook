{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Dynamic Pages using Beautifulsoup\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I have been working as a freelancer for sometimes now. However, my last client challenged me after working with him for 2 years he asked me this question. \"Wes, you are a good data analysts, but when I check you online I find nothing, what's the issue?\" This question really challenged me and started looking up my client online. Guess what I found 99% of his publications have passed through my analysis skills. Infact, some articles he publish without even changing a word or an approach. Okay I'm not suing him for that because he paid me well for the effort, implying the work now belong to him. This challenged me alot and in this short jupyter notebook I want to share my data scraping knowledge with others. \n",
    "Data scraping is illegal if you keep on hitting a server multiple times from the same computer or if you extract data for commercial purposes. Other companies or website donot want their data scraped for competition purpose and so on. However, the site I'm going to extract the data haven't given me any authority to do so, they can also look at my code and prevent others from using it to obtain data from their site. \n",
    "\n",
    "*Discalimer* I'm extracting this data only for educational purposes. \n",
    "\n",
    "## Outline of Extraction\n",
    "\n",
    "1. Import the libraries needed\n",
    "2. Extract the data\n",
    "3. Save the data as excel file\n",
    "\n",
    "### 1. Import the needed libraries\n",
    "\n",
    "The libraies that I'm going to use include: `BeautifulSoup` from `bs4`, `request`, `pandas`, `time`, and `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more infomration on these libries see [BeautifulSoup]('https://www.crummy.com/software/BeautifulSoup/doc'), [request]('https://realpython.com/python-requests/#:~:text=The%20requests%20library%20is%20the,consuming%20data%20in%20your%20application.'), [pandas]('https://pandas.pydata.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have an header for our scraping algorithm to avoid being turned down by the webpage. This helps us to behave like a normal browers. Next the base url to be scraped. The code is as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 0\n",
      "scraping 1\n",
      "Pages scraped saving data\n",
      "data saved\n",
      "you can now view the data\n"
     ]
    }
   ],
   "source": [
    "# Garrage name\n",
    "Name = []\n",
    "# Email address\n",
    "Email = []\n",
    "# website address\n",
    "Web = []\n",
    "# Header address\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "headers1 = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0'}\n",
    "# Url\n",
    "url_base = 'https://www.agvs-upsa.ch/de/verband/mitgliederverzeichnis/liste?distance&page='\n",
    "# We only need to scrape first 5 pages to avoid overloding the server\n",
    "for page in range(0,2):\n",
    "    print(\"scraping \" + str(page))\n",
    "    url = url_base + str(page)\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "    name1 = soup.select('span.field-content')\n",
    "    for nam in name1:\n",
    "        Name.append(re.sub(r\" ?\\([^)]+\\)\", \"\", nam.get_text()))\n",
    "    email1 = soup.select('div.field-content  a[href^=mailto]')\n",
    "    for em in email1:\n",
    "        Email.append(em.get_text())\n",
    "    web1 = soup.select('div.field-content  a[href^=http]')\n",
    "    for we in web1:\n",
    "        Web.append(we.get_text())\n",
    "print(\"Pages scraped saving data\")\n",
    "data = list(zip(Name, Email, Web))\n",
    "results = pd.DataFrame(data, columns = ['NAME', 'E-MAIL', 'WEB-SITE']) \n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('GarageData.xlsx', engine='xlsxwriter')\n",
    "print(\"data saved\")\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "results.to_excel(writer, sheet_name='Information')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "print(\"you can now view the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this code you will create an excel document with the required information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
